{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proportion of edits done by IP users\n",
    "\n",
    "This is part of [T231605](https://phabricator.wikimedia.org/T231605), covering the first part that looks at the proportion of edits done by IP users across projects.\n",
    "\n",
    "Specifically:\n",
    "\n",
    "* How many edits are made by IP editors on our projects?\n",
    "\n",
    "It would be preferable to have this broken down by project (e.g. Wikipedias, Wikisource, etcâ€¦)\n",
    "\n",
    "## Breaking it down by wiki and project group\n",
    "\n",
    "I'll reuse [Neil's code for getting canonical data about wikis](https://github.com/wikimedia-research/canonical-data/blob/master/generation/wikis.ipynb) to be able to get data by project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from wmfdata import mariadb, hive\n",
    "\n",
    "import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dblist(list_name):\n",
    "    list_url = (\"https://noc.wikimedia.org/conf/dblists/\" + list_name + \".dblist\")\n",
    "    list_content = requests.get(list_url).text.split(\"\\n\")\n",
    "    return pd.Series(list_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikis = mariadb.run(\"\"\"\n",
    "select\n",
    "    site_global_key as database_code,\n",
    "    concat(trim(leading \".\" from reverse(site_domain))) as domain_name,\n",
    "    site_group as database_group,\n",
    "    site_language as language_code\n",
    "from enwiki.sites\n",
    "\"\"\", \"enwiki\").sort_values(\"database_code\").set_index(\"database_code\")\n",
    "\n",
    "wikis.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_urls = [\n",
    "    \"https://raw.githubusercontent.com/wikimedia/mediawiki-extensions-cldr/master/CldrNames/CldrNamesEn.php\",\n",
    "    \"https://raw.githubusercontent.com/wikimedia/mediawiki-extensions-cldr/master/LocalNames/LocalNamesEn.php\"\n",
    "]\n",
    "\n",
    "def get_lang_names(url):\n",
    "    r = requests.get(url)\n",
    "    m = re.search(r\"languageNames = (\\[[\\s\\S]+?\\])\", r.text)\n",
    "    php_ln = m.group(1)\n",
    "    \n",
    "    json_ln = php_ln\n",
    "    repl = [\n",
    "        # Convert from PHP array format to JSON\n",
    "        (\" =>\", \":\"),\n",
    "        (\"\\[\", \"{\"),\n",
    "        (\"\\]\", \"}\"),\n",
    "        # Trailing commas will cause problems\n",
    "        (\",\\n}\", \"\\n}\"),\n",
    "        # ...so will single quotes\n",
    "        (\"'\", '\"'),\n",
    "        # ...and comments\n",
    "        (r\"/\\*[\\s\\S]*?\\*/\", \"\"),\n",
    "        (r\"#(.*?)\\n\", \"\"),\n",
    "        # One hack to deal with a single quote in a language name\n",
    "        ('O\"odham', \"O'odham\")\n",
    "    ]\n",
    "    for old, new in repl:\n",
    "        json_ln = re.sub(old, new, json_ln)\n",
    "    \n",
    "    py_ln = json.loads(json_ln)\n",
    "    return py_ln\n",
    "\n",
    "langs = {}\n",
    "for url in lang_urls:\n",
    "    langs.update(get_lang_names(url))\n",
    "\n",
    "# Add languages not included in the CLDR files\n",
    "langs.update({\n",
    "    \"als\": \"Alsatian\",\n",
    "    \"atj\": \"Atikamekw\",\n",
    "    \"diq\": \"Zazaki\",\n",
    "    \"fiu-vro\": \"VÃµro\",\n",
    "    \"map-bms\": \"Banyumasan\",\n",
    "    \"nah\": \"Nahuatl\",\n",
    "    \"pih\": \"Norfuk-Pitkern\",\n",
    "    \"rmy\": \"Vlax Romani\",\n",
    "    \"simple\": \"Simple English\"\n",
    "})\n",
    "\n",
    "wikis[\"language_name\"] = wikis[\"language_code\"].apply(langs.get)\n",
    "\n",
    "wikis.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "closed = get_dblist(\"closed\")\n",
    "private = get_dblist(\"private\")\n",
    "\n",
    "def apply_to_index(df, true_list, true_label, false_label):\n",
    "    idx_ser = df.index.to_series()\n",
    "    return idx_ser.isin(true_list).apply(lambda x: true_label if x else false_label)\n",
    "\n",
    "wikis = (\n",
    "    wikis\n",
    "    .assign(\n",
    "        status=lambda df: apply_to_index(df, closed, \"closed\", \"open\"),\n",
    "        visbility=lambda df: apply_to_index(df, private, \"private\", \"public\")\n",
    "    )\n",
    ")\n",
    "\n",
    "wikis.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this data about wikis, we can then limit the analysis to only open and publicly available wikis.\n",
    "\n",
    "## Determining time frame\n",
    "\n",
    "We'd like to be able to get an overview as well as likely drilling down a bit. Therefore, I'll gather data for the previous year (September 2018 through August 2019) on a monthly basis. This data can then be combined to get quarterly, semi-annualy, and annual statistics. We can also use it to get range values for each wiki/project.\n",
    "\n",
    "## Definitions\n",
    "\n",
    "Let's make some definitions and describe some limitations.\n",
    "\n",
    "1. We'll only measure this for wikis that are open and public per their definition in the `wikis` DataFrame we have gathered.\n",
    "2. We'll use the Data Lake to gather data, meaning that the Mediawiki History table is our authoritative source of data.\n",
    "3. We'll ignore all edits where `event_user_id IS NULL`, meaning the user has been revision deleted.\n",
    "4. We'll use the `event_user_is_anonymous` column to determine if an edit is by a non-registered user. If the value is `true` the user is non-registered, and `false` means the user is registered.\n",
    "5. We'll count bot edits separately by checking whether `event_user_is_bot_by`  or `event_user_is_bot_by_historical` is set. This means that a user that at some point has been labelled a bot will always be labelled a bot. While this is more in line with enwiki policy (bot accounts are separate, identifiable, and approved) rather than other wikis (e.g. where bots can run without a bot flag), we see it as unlikely that active accounts change their status on a regular basis.\n",
    "\n",
    "Update: it looks like `event_user_id` cannot be used in this way, the column is always `NULL` for anonymous edits. I've filed [T232171](https://phabricator.wikimedia.org/T232171) about this, not sure if my understanding of the documentation is correct.\n",
    "Update: the documentation had a bug, which is now fixed. I've switched to using `event_user_is_anonmyous` and updated the description above.\n",
    "\n",
    "I can identify revisions where the user has been deleted using `revision_deleted_parts`, it will contain `user` if that info is deleted. Then, I should be able to use `event_user_is_anonymous = true/false` to separate between edits by IPs and registered users.\n",
    "\n",
    "## Data gathering\n",
    "\n",
    "Based on the definitions above, we can write the Hive query below to gather data for all wikis using the Data Lake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edit_count_query = '''\n",
    "SELECT wiki_db, DATE_FORMAT(event_timestamp, \"yyyy-MM-01\") AS log_month,\n",
    "  SUM(1) AS num_total_edits,\n",
    "  SUM(IF(event_user_is_anonymous = true, 1, 0)) AS num_ip_edits,\n",
    "  SUM(IF(event_user_is_anonymous = false, 1, 0)) AS num_reg_edits,\n",
    "  SUM(IF(event_user_is_anonymous = false\n",
    "         AND (SIZE(event_user_is_bot_by) > 0\n",
    "              OR SIZE(event_user_is_bot_by_historical) > 0), 1, 0)) AS num_bot_edits\n",
    "FROM wmf.mediawiki_history\n",
    "WHERE snapshot = \"{snapshot}\"\n",
    "AND event_entity = \"revision\"\n",
    "AND event_type = \"create\"\n",
    "AND array_contains(revision_deleted_parts, 'user') = false -- skip revisions w/deleted user info\n",
    "AND event_timestamp >= \"{start_date}\"\n",
    "AND event_timestamp < \"{end_date}\"\n",
    "AND wiki_db IN ({wiki_list})\n",
    "GROUP BY wiki_db, DATE_FORMAT(event_timestamp, \"yyyy-MM-01\")\n",
    "LIMIT 50000\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikis.loc[(wikis.status == 'open') & (wikis.visbility == 'public')].index.tolist()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## NOTE: we limit the data gathering of edit counts to only open and public wikis here.\n",
    "allprojects_edit_counts = hive.run(\n",
    "    edit_count_query.format(\n",
    "        snapshot = '2019-08',\n",
    "        start_date = '2018-09-01',\n",
    "        end_date = '2019-09-01',\n",
    "        wiki_list = ', '.join(\n",
    "            ['\"{}\"'.format(w) for w in wikis.loc[(wikis.status == 'open') &\n",
    "                                                 (wikis.visbility == 'public')].index.tolist()])\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allprojects_edit_counts.loc[allprojects_edit_counts.wiki_db == 'enwiki']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yearly stats\n",
    "\n",
    "Over the past year, how many edits did we have across our projects (wikipedias, wikisources, etc), and of those, how many were IPs, registered users, and bots?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikis.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yearly_stats = (allprojects_edit_counts.set_index('wiki_db')\n",
    "                .merge(wikis[['database_group']], left_index = True, right_index = True)\n",
    "                .reset_index().groupby('database_group').sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: it might be confusing to see all the wikimedia projects in the yearly stats. Those are the local chapter wikis. Would perhaps be useful if they had their own database group designation (e.g. wikimedia, but that might be reserved for the WMF).\n",
    "\n",
    "We want yearly counts and percentages. But, `num_total_edits` doesn't necessarily equal `num_ip_edits + num_reg_edits`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yearly_stats.loc[yearly_stats.num_total_edits == yearly_stats.num_ip_edits + yearly_stats.num_reg_edits]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've cleared the output, but there are several projects where it does match. While I can ponder on the conditions for why these might not equal each other, I'll instead use `num_total_edits` as the denominator, and note that things might not add up to 100.0%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yearly_stats['prop_ip'] = 100 * yearly_stats.num_ip_edits / yearly_stats.num_total_edits\n",
    "yearly_stats['prop_reg'] = 100 * yearly_stats.num_reg_edits / yearly_stats.num_total_edits\n",
    "yearly_stats['prop_bots'] = 100 * yearly_stats.num_bot_edits / yearly_stats.num_total_edits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yearly_stats.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the projects we're interested in? I propose the following, although I can see the Outreach wiki also being one of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projects = ['commons', 'mediawiki', 'meta', 'wikibooks', 'wikidata', 'wikinews', 'wikipedia',\n",
    "            'wikiquote', 'wikisource', 'wikiversity', 'wikivoyage', 'wiktionary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    tabulate.tabulate(\n",
    "        yearly_stats.loc[projects]\n",
    "        [['num_total_edits', 'num_ip_edits', 'prop_ip', 'num_reg_edits', 'prop_reg',\n",
    "          'num_bot_edits', 'prop_bots']]\n",
    "        .rename(\n",
    "            columns = {'num_total_edits' : 'N Total edits',\n",
    "                       'num_ip_edits' : 'N IP edits',\n",
    "                       'prop_ip' : 'IP proportion',\n",
    "                       'num_reg_edits' : 'N Registered edits',\n",
    "                       'prop_reg' : 'Registered proportion',\n",
    "                       'num_bot_edits' : 'N Bot edits',\n",
    "                       'prop_bots' : 'Bot proportion'}\n",
    "        ), headers = 'keys', tablefmt = 'github',\n",
    "        floatfmt=[\"f\", \".0f\", \".0f\", \".1f\", \".0f\", \".1f\", \".0f\", \".1f\"]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    tabulate.tabulate(\n",
    "        yearly_stats.loc[projects]\n",
    "        [['num_total_edits', 'num_ip_edits', 'prop_ip', 'num_reg_edits', 'prop_reg',\n",
    "          'num_bot_edits', 'prop_bots']]\n",
    "        .rename(\n",
    "            columns = {'num_total_edits' : 'N Total edits',\n",
    "                       'num_ip_edits' : 'N IP edits',\n",
    "                       'prop_ip' : 'IP proportion',\n",
    "                       'num_reg_edits' : 'N Registered edits',\n",
    "                       'prop_reg' : 'Registered proportion',\n",
    "                       'num_bot_edits' : 'N Bot edits',\n",
    "                       'prop_bots' : 'Bot proportion'}\n",
    "        ), headers = 'keys', tablefmt = 'mediawiki',\n",
    "        floatfmt=[\"\", \".0f\", \".0f\", \".1f\", \".0f\", \".1f\", \".0f\", \".1f\"]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## I need this so I can replace all the scientific notation with the full integers.\n",
    "yearly_stats.loc[projects]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Per-project statistics\n",
    "\n",
    "For each of the overarching projects, we want to split it out by language, calculate monthly averages and provide min/max values as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_project_stats = (allprojects_edit_counts.set_index('wiki_db')\n",
    "                     .merge(wikis[['database_group', 'language_name']],\n",
    "                            left_index = True, right_index = True)\n",
    "                     .reset_index().rename(columns = {'index': 'wiki_db'}))\n",
    "per_project_stats = per_project_stats.loc[per_project_stats.database_group.isin(projects)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_project_stats.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monthly_overview(group):\n",
    "    aggs = {\n",
    "        'mean_total_edits' : group.num_total_edits.mean(),\n",
    "        'mean_ip_edits' : group.num_ip_edits.mean(),\n",
    "        'mean_reg_edits' : group.num_reg_edits.mean(),\n",
    "        'mean_bot_edits' : group.num_bot_edits.mean(),\n",
    "        'mean_prop_ip_edits' : 100 * group.num_ip_edits.mean() / group.num_total_edits.mean(),\n",
    "        'min_prop_ip_edits' : 100 * np.min(group.num_ip_edits / group.num_total_edits),\n",
    "        'max_prop_ip_edits' : 100 * np.max(group.num_ip_edits / group.num_total_edits),\n",
    "        'mean_prop_reg_edits' : 100 * group.num_reg_edits.mean() / group.num_total_edits.mean(),\n",
    "        'min_prop_reg_edits' : 100 * np.min(group.num_reg_edits / group.num_total_edits),\n",
    "        'max_prop_reg_edits' : 100 * np.max(group.num_reg_edits / group.num_total_edits),\n",
    "        'mean_prop_bot_edits' : 100 * group.num_bot_edits.mean() / group.num_total_edits.mean(),\n",
    "        'min_prop_bot_edits' : 100 * np.min(group.num_bot_edits / group.num_total_edits),\n",
    "        'max_prop_bot_edits' : 100 * np.max(group.num_bot_edits / group.num_total_edits),\n",
    "    }\n",
    "    return(pd.Series(aggs, index = aggs.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_project_stats_agg = per_project_stats.groupby(['database_group', 'language_name']).apply(monthly_overview)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_project_stats_agg = per_project_stats_agg.reset_index().set_index('database_group')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: It might be possible to get numbers formatted properly in MediaWiki by converting from a number to a string (so it's `\"{{formatnum:\\d+}}\"`) and then perhaps use `stralign` in `tabulate.tabulate` to right-align the relevant columns. Will have to try that for a later iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_project_group(df, group_name, table_format = 'mediawiki', precision = 1):\n",
    "    '''\n",
    "    From the aggregated project statistics, print out a table of the monthly average statistics\n",
    "    using the given table format (default is a wikitable). Expects the DataFrame `df` to have\n",
    "    an index so that `group_name` matches.\n",
    "    '''\n",
    "    \n",
    "    column_order = ['language_name', 'mean_total_edits', 'mean_ip_edits', 'mean_prop_ip_edits',\n",
    "                    'min_prop_ip_edits', 'max_prop_ip_edits']\n",
    "    \n",
    "    column_renaming = {\n",
    "        'language_name' : 'Language',\n",
    "        'mean_total_edits' : 'Monthly total average',\n",
    "        'mean_ip_edits' : 'Monthly IP average',\n",
    "        'mean_prop_ip_edits' : 'IP % average',\n",
    "        'min_prop_ip_edits' : 'Min. IP %',\n",
    "        'max_prop_ip_edits' : 'Max. IP %',\n",
    "    }\n",
    "\n",
    "    print(\n",
    "        tabulate.tabulate(\n",
    "            df.loc[group_name].reset_index()[column_order].rename(columns = column_renaming),\n",
    "            headers = 'keys', tablefmt = table_format, showindex = False, numalign = 'right',\n",
    "            floatfmt = ['', '.1f', '.1f', '.1f', '.1f', '.1f']\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_project_group(per_project_stats_agg, ['commons'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_project_group(per_project_stats_agg, ['mediawiki'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_project_group(per_project_stats_agg, ['meta'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_project_group(per_project_stats_agg, ['wikidata'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_project_group(per_project_stats_agg, 'wikibooks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_project_group(per_project_stats_agg, 'wikinews')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_project_group(per_project_stats_agg, 'wikipedia')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_project_group(per_project_stats_agg, 'wikiquote')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_project_group(per_project_stats_agg, 'wikisource')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_project_group(per_project_stats_agg, 'wikiversity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_project_group(per_project_stats_agg, 'wikivoyage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_project_group(per_project_stats_agg, 'wiktionary')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
